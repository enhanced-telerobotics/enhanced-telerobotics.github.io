<!doctype html><html class="no-js" lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Vision-based Force Estimation and Haptic Feedback using Neural Networks</title><link rel="stylesheet" type="text/css" href="https://enhanced-telerobotics.github.io/assets/css/styles_feeling_responsive.css"> <script src="https://enhanced-telerobotics.github.io/assets/js/modernizr.min.js"></script> <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.5.18/webfont.js"></script> <script> WebFont.load({ google: { families: [ 'Lato:400,700,400italic:latin', 'Volkhov::latin' ] } }); </script> <noscript><link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic%7CVolkhov' rel='stylesheet' type='text/css'> </noscript><meta name="description" content="Forcep tip force sensing in robot-assisted minimally invasive surgery is challenging due to strict requirements for miniaturization, biocompatibility, and sterilizability. Indirect force estimation is a promising method to measure forces while circumventing these constraints. Much like how humans can estimate forces visually, neural networks can attempt to do something similar. However, there are concerns as to the generalizability of these methods as well as the relative importance of visual information compared to robot kinematic state information as inputs. We characterize the performance of vision-based neural networks with these considerations in mind and quantify the quality of the closed-loop haptic feedback they can provide to the operator."><link rel="canonical" href="https://enhanced-telerobotics.github.io/research/NNForceEstimation/"><meta property="og:title" content="Vision-based Force Estimation and Haptic Feedback using Neural Networks"><meta property="og:description" content="Forcep tip force sensing in robot-assisted minimally invasive surgery is challenging due to strict requirements for miniaturization, biocompatibility, and sterilizability. Indirect force estimation is a promising method to measure forces while circumventing these constraints. Much like how humans can estimate forces visually, neural networks can attempt to do something similar. However, there are concerns as to the generalizability of these methods as well as the relative importance of visual information compared to robot kinematic state information as inputs. We characterize the performance of vision-based neural networks with these considerations in mind and quantify the quality of the closed-loop haptic feedback they can provide to the operator."><meta property="og:url" content="https://enhanced-telerobotics.github.io/research/NNForceEstimation/"><meta property="og:locale" content="en_EN"><meta property="og:type" content="website"><meta property="og:site_name" content="ERIE Lab at CWRU"><meta property="og:image" content="/images/nn_force_est_main.png"><link type="text/plain" rel="author" href="https://enhanced-telerobotics.github.io/humans.txt"><link rel="icon" sizes="32x32" href="https://enhanced-telerobotics.github.io/assets/img/favicon-32x32.png"><link rel="icon" sizes="192x192" href="https://enhanced-telerobotics.github.io/assets/img/touch-icon-192x192.png"><link rel="apple-touch-icon-precomposed" sizes="180x180" href="https://enhanced-telerobotics.github.io/assets/img/apple-touch-icon-180x180-precomposed.png"><link rel="apple-touch-icon-precomposed" sizes="152x152" href="https://enhanced-telerobotics.github.io/assets/img/apple-touch-icon-152x152-precomposed.png"><link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://enhanced-telerobotics.github.io/assets/img/apple-touch-icon-144x144-precomposed.png"><link rel="apple-touch-icon-precomposed" sizes="120x120" href="https://enhanced-telerobotics.github.io/assets/img/apple-touch-icon-120x120-precomposed.png"><link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://enhanced-telerobotics.github.io/assets/img/apple-touch-icon-114x114-precomposed.png"><link rel="apple-touch-icon-precomposed" sizes="76x76" href="https://enhanced-telerobotics.github.io/assets/img/apple-touch-icon-76x76-precomposed.png"><link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://enhanced-telerobotics.github.io/assets/img/apple-touch-icon-72x72-precomposed.png"><link rel="apple-touch-icon-precomposed" href="https://enhanced-telerobotics.github.io/assets/img/apple-touch-icon-precomposed.png"><meta name="msapplication-TileImage" content="https://enhanced-telerobotics.github.io/assets/img/msapplication_tileimage.png"><meta name="msapplication-TileColor" content="#fabb00"> <script async src="https://www.googletagmanager.com/gtag/js?id=G-N5Y5DWZZGV"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-N5Y5DWZZGV'); </script><body id="top-of-page" class="post"><div id="navigation" class="sticky"><nav class="top-bar" role="navigation" data-topbar data-options="scrolltop: false"><ul class="title-area"><li class="name"><h1 class="hide-for-large-up"><a href="https://enhanced-telerobotics.github.io"> ERIE Lab at CWRU</a></h1><li class="toggle-topbar toggle-topbar-click menu-icon"><a><span>Nav</span></a></ul><section class="top-bar-section"><ul class="left"><li><a href="https://enhanced-telerobotics.github.io/">Home</a><li class="divider"><li><a href="https://enhanced-telerobotics.github.io/team/">Team</a><li class="divider"><li><a href="https://enhanced-telerobotics.github.io/research-list/">Research</a><li class="divider"><li><a href="https://enhanced-telerobotics.github.io/publications/">Publications</a><li class="divider"><li class="has-dropdown"> <a href="https://enhanced-telerobotics.github.io/teaching/">Teaching</a><ul class="dropdown"><li><a href="https://enhanced-telerobotics.github.io/ECSE600/">ECSE600</a></ul><li class="divider"><li class="has-dropdown"> <a href="https://enhanced-telerobotics.github.io/news/">News</a><ul class="dropdown"><li><a href="https://enhanced-telerobotics.github.io/news/archive/">Archive</a></ul><li class="divider"></ul><ul class="right"><li class="divider"><li><a href="https://enhanced-telerobotics.github.io/contact/">Contact</a></ul></section></nav></div><div id="masthead-no-image-header"><div class="row"><div class="small-12 columns"> <a id="logo" href="https://enhanced-telerobotics.github.io/" title="ERIE Lab at CWRU – Enhanced teleRobotic Interfaces at CWRU"> <img src="https://enhanced-telerobotics.github.io/assets/img/logo.png" alt="ERIE Lab at CWRU – Enhanced teleRobotic Interfaces at CWRU"> </a></div></div></div><div class="row t30"><div class="medium-8 columns medium-offset-2 end"><article itemscope itemtype="http://schema.org/Article"><header><figure> <img src="/images/nn_force_est_main.png" width="970" alt="Vision-based Force Estimation and Haptic Feedback using Neural Networks" itemprop="image"></figure><div itemprop="name"><p class="subheadline">Research<h1>Vision-based Force Estimation and Haptic Feedback using Neural Networks</h1></div></header><p class="teaser" itemprop="description"> Forcep tip force sensing in robot-assisted minimally invasive surgery is challenging due to strict requirements for miniaturization, biocompatibility, and sterilizability. Indirect force estimation is a promising method to measure forces while circumventing these constraints. Much like how humans can estimate forces visually, neural networks can attempt to do something similar. However, there are concerns as to the generalizability of these methods as well as the relative importance of visual information compared to robot kinematic state information as inputs. We characterize the performance of vision-based neural networks with these considerations in mind and quantify the quality of the closed-loop haptic feedback they can provide to the operator.<div itemprop="articleSection"><p>Primary Researcher: Shuyuan Yang<p><br /> <br /><div class="row"><div class="medium-6 columns"> <img src="/images/force_trajectory.gif" alt="Offline NN predictions" /></div><div class="medium-6 columns"> Our neural networks are capable of estimating interaction forces offline. In the figure below, the black line is the ground truth force while the green, red, and blue lines are our vision-only, state-only, and vision+state neural networks. We included two benchmarks, an RNN (purple) as well as a physics-based dynamic model (gold).</div></div><p><br /> <br /><div class="row"><p> We also performed a gradient class activation to understand what visual features were being used by the neural networs to estimate force. Here we see from left to right that in the x-, y-, and z-directions, the neural networks are looking at the shadows and deformation of the silicone material. <img src="/images/gradcam.gif" alt="NN gradcam" style="margin:auto;display:block;width=50%;" /></div><p><br /> <br /> We have posted part of the dataset from <a class="citation" href="#ChuaICRA2021">[1]</a> online. This can be access via our github repo page <a href="https://github.com/enhanced-telerobotics/single_psm_manipulation_dataset">here</a>. <br /> <br /><h2 id="published-works">Published Works</h2><p><span id="yangJMRR2024">S. Yang, M. H. Le, K. R. Golobish, J. C. Beaver, and Z. Chua, “Vision-Based Force Estimation for Minimally Invasive Telesurgery Through Contact Detection and Local Stiffness Models,” <i>Journal of Medical Robotics Research</i>, 2024.</span><p><span id="ChuaIROS2022">Z. Chua and A. M. Okamura, <a href="http://arxiv.org/abs/2109.11488">“Characterization of Real-time Haptic Feedback from Multimodal Neural Network-based Force Estimates during Teleoperation,”</a> in <i>IEEE/RSJ International Conference on Intelligent Robots and Systems</i>, 2022.</span><p><span id="ChuaICRA2021">Z. Chua, A. M. Jarc, and A. M. Okamura, <a href="http://arxiv.org/abs/2011.02112">“Toward Force Estimation in Robot-Assisted Surgery using Deep Learning with Vision and Robot State,”</a> in <i>IEEE International Conference on Robotics and Automation</i>, 2021, pp. 12335–12341.</span><h2 class="t60" id="other-research">Other Research</h2><ul class="side-nav"><li><a href="https://enhanced-telerobotics.github.io/research/OpenSourceForceSensor/">Research &middot; <strong>Open Source Force Sensors for Minimally Invasive Surgical Robotics Research</strong></a><li><a href="https://enhanced-telerobotics.github.io/research/NNForceEstimation/">Research &middot; <strong>Vision-based Force Estimation and Haptic Feedback using Neural Networks</strong></a><li><a href="https://enhanced-telerobotics.github.io/research/VisuohapticPriorExperience/">Research &middot; <strong>Learning Visual Force Estimation during Teleoperation of a Surgical Robot</strong></a><li class="text-right"><a href="https://enhanced-telerobotics.github.io/news/archive/"><strong>More ›</strong></a></ul></div></article></div></div><div id="up-to-top" class="row"><div class="small-12 columns" style="text-align: right;"> <a class="iconfont" href="#top-of-page">&#xf108;</a></div></div><footer id="footer-content" class="bg-grau"><div id="subfooter"><nav class="row"><section id="subfooter-left" class="small-12 medium-6 columns credits"><p> © Zonghe Chua at Case Western Reserve University <br> made with <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> based on <a href="http://phlow.github.io/feeling-responsive/">Feeling Responsive</a>.</section><section id="subfooter-right" class="small-12 medium-6 columns"><ul class="inline-list social-icons"></ul></section></nav></div></footer><script src="https://enhanced-telerobotics.github.io/assets/js/javascript.min.js"></script>
